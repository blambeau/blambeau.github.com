I've been talking about _logical data independence_ @{logical_data_independence}{here}, about source code independence @{book#how}{there}, and so on. It's probably time to provide some lecture about _independence_ in the large when talking about computer science.

I wanted to start this post with a quick review of google results for "computer science independence" or "software architecture independence". I have to admit that the results are not really convincing...

In my opinion, one of the major weaknesses of computer science nowadays is the lack of theoretical abstraction. For instance, good students of mine know about java interfaces, but they don't have any insight of general principles underlying the _reason of introducing interfaces_... Whatever your profile, *knowing good principles is always better than knowing practical tips and tricks !*

h3. What does independence mean?

Independence in computer science is both about _change_ and about _abstraction_. More precisely, _reaching independence_ means _using the good abstractions_ to allow the software not to be too much hurted by _changes_. 

h3. What kind of change?

Changes may be numerous and of different nature. The list below is a open-ended proposal, don't hesitate to ask questions or to start a discussion about practical examples.

|<code>Assumptions</code>|Your software always makes a lot of assumptions about its environment, some of them being implicit, others being well known and/or documented. The second are not so hurting... while the former are killers.|
|<code>Requirements</code>|Probably the worst... changing requirements (what the software is supposed to do) often means changing the implementation in depth. How to react to changing requirements?|
|<code>Evolution</code>|New features? Great! New features generally means that the software is being adopted by its users. But how to implement new features without hurting existing ones?|
|<code>Third party</code>|Almost all softwares use third party libraries/tools/devices. Those components also change and these changes may affect portability/stability/robustness of your own software. How to get away from third party changes?|
|<code>Optimization</code>|People want softwares to act fast, and always faster. Optimizing means _changing the base code_ to handle such requests. Optimization is too often premature, and even dangerous if it leads to breaking features. So what?|

h3. How to react in presence of changes?

Maybe unfortunate... but computer science - and development in particular - is an art before being a science. There is no cookbook for designing architectures and creating softwares that prevent hurting changes. However, in my experience, there exists some guidelines that I discuss briefly here. Concrete examples are spread over this blog and the related open-source libraries... I'm affraid, it's the best I can do for now ;-)

I've said previously that independence is related to _abstraction_. It's true. In practice however, there's two ways to handle _changes_. The pragmatic way: _accept them and react accordingly_. The theoretical one: _prevent changes_ with good requirements, good architecture and so on. These two approaches are *not* antagonistic. Personally, I apply a lot of "patterns" to handle changes... They are all practical things (something that you apply for real), but many of them are inspired by sound principles (something more abstract that you know the merits). I list them below somewhat theoretically, I'll try to write papers detailing some of them on practical examples later.

|<code>Testing</code>|Are you already applying _test driven development_? No? *Give it a chance*... *now!* It helps you making <u>assumptions</u> explicit, thinking about <u>requirements</u>, detecting <u>third party</u> changes and not fearing <u>optimization refactorings</u>. For instance, when I start *learning a new third party library, I write tests* and assert my own understanding of that library. Such sandbox tests stay alive with your software, *provide a shield about third party changes* and  ensure that *your assumptions stay correct*.|
|<code>Uncertainty</code>|When I'm not sure about something (related to <u>assumptions</u>, <u>requirements</u> and <u>third party</u>) I encapsulate my uncertainty inside a given function/class/method/package/whatever with an extremely precise (and strong) specification. Some examples: <ul><li>The documentation of a third party method behavior is vague? Encapsulate calls to it with something stronger!</li><li>Hesitating between interfacing with an external library, copying (open-source) code from it or reimplementing features of interest? Write a facade in front of the features you use.</li><li>Not sure how a method is supposed to behave in a specific situation - maybe because the problem it solves is not completely specified yet? Detect the situation and raise an error: for the caller, _having a strong post-condition_ (an error is raised in that case) is always better than _hoping it will pass_.</li></ul>These techniques are extremely powerful: in all cases, you replace vagueness and uncertainty by strong certitudes. Detecting and reacting to future changes will be a lot easier.|
|<code>Preprocessing</code>|An important source of change is _input change_ (related to _requirements_ and _evolution_) |
