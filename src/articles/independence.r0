I've been talking about _logical data independence_ @{logical_data_independence}{here}, about source code independence @{book#how}{there}, and so on. It's probably time to provide some lecture about _independence_ in the large when talking about computer science.

I wanted to start this post with a quick review of google results for "computer science independence" or "software architecture independence". I have to admit that the results are not really convincing...

In my opinion, one of the major weaknesses of computer science nowadays is the lack of theoretical abstraction. For instance, good students of mine know about java interfaces, but they don't have any insight of general principles underlying the _reason of introducing interfaces_... Whatever your profile, *knowing good principles is always better than knowing practical tips and tricks !*

h3. What does independence mean?

Independence in computer science is both about _change_ and about _abstraction_. More precisely, _reaching independence_ means _using the good abstractions_ to allow the software not to be too much hurted by _changes_. 

h3. What kind of change?

Changes may be numerous and of different nature. The list below is a open-ended proposal, don't hesitate to ask questions or to start a discussion about practical examples.

|<code>Assumptions</code>|Your software always makes a lot of assumptions about its environment, some of them being implicit, others being well known and/or documented. The second are not so hurting... while the former are killers.|
|<code>Requirements</code>|Probably the worst... changing requirements (what the software is supposed to do) often means changing the implementation in depth. How to react to changing requirements?|
|<code>Evolution</code>|New features? Great! New features generally means that the software is being adopted by its users. But how to implement new features without hurting existing ones?|
|<code>Third party</code>|Almost all softwares use third party libraries/tools/devices. Those components also change and these changes may affect portability/stability/robustness of your own software. How to get away from third party changes?|
|<code>Optimization</code>|People want softwares to act fast, and always faster. Optimizing means _changing the base code_ to handle such requests. Optimization is too often premature, and even dangerous if it leads to breaking features. So what?|

h3. How to react in presence of changes?

Maybe unfortunate... but computer science - and development in particular - is an art before being a science. There is no cookbook for designing architectures and creating softwares that prevent hurting changes. However, in my experience, there exists some guidelines that I discuss briefly here. Concrete examples are spread over this blog and the related open-source libraries... I'm affrais, it's the best I can do for now ;-)

I've said previously that independence is related to _abstraction_. It's true. In practice however, there's two ways to handle _changes_. The pragmatic way: _accept them and react accordingly_. The theoretical one: _prevent yourself from changes_. These two  approaches are perfectly compatible: too many people makes the error of enjoying the war between theory and practice. Let discuss the practical ones:

|<code>Testing</code>|Are you already applying _test driven development_? No? *Give it a chance*... *now!* It helps you making <u>assumptions</u> explicit, thinking about <u>requirements</u>, detecting <u>third party</u> changes an starting strong <u>optimization refacorings</u>. For instance, when I start *learning a new third party library, I write tests* and assert my own understanding of that library. Such sandbox tests should always stay alive with your software codebase because *they provide a shield about third party changes* and they ensure that *your assumptions are correct*. For the second in particular, you'll discover that your assumptions are often wrong.|
|<code>Uncertainty</code>|If you're not sure about something (related to <u>assumptions</u>; typical situation: when the documentation of a third library documentation is vague about it's behavior in a given situation), you *have to* encapsulate your uncertainty inside a given function/class/method/package/whatever with an extremely precise (and closed) specification. When you'll gain confidence about the library behavior or better understanding of valid assumption(s), you'll have the opportunity to weaken/open your specification. How? In that method, check the arguments, assert what seems natural, raise/throw exceptions on any unexpected situation, and so on. The software will probably fail but the failure reason will be much clearer.|