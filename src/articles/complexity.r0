Recently, I've seen the famous presentation "ZOMG WHY IS THIS CODE SO SLOW":http://www.slideshare.net/tenderlove/zomg-why-is-this-code-so-slow that Aaron Patterson (aka "tenderlove":http://tenderlovemaking.com/) gave at RubyConf2010. The talk is about benchmarking and performance. I must confess being a bit embarrassed about what I'm going to say here (Aaron is a really famous ruby hacker, member of both ruby-core and rails-core teams, developer of excellent ruby tools like "ARel":https://github.com/rails/arel, "mechanize":https://github.com/tenderlove/mechanize, and so on.) but...  I'm a bit... how to say?... hum.

Before explaining why, I would like to be clear: I'm not arguing here that Aaron badly understands basic complexity theory. Neither do I argue that the way he conducted his benchmarking experience is wrong. Anyway, some examples in the slides seem wrong at worse and extremely confusing at best. So I dare ask: _Why having so great ruby speakers if they insufflate bad practices, even if unintentionally (see later)?_


h3. Benchmarking and asserting execution time

The example below is given at slide 104, when introducing the "minitest/benchmark":https://github.com/seattlerb/minitest library (I must add that, at the time of writing, a very similar example is given in "minitest's README":https://github.com/seattlerb/minitest/blob/81fe0a56f5dd29036e3bec107cca48a136c42470/README.txt#L110-119). The example is quite easy to understand:

# The method @fib@ computes the n-th "fibonacci number":http://en.wikipedia.org/wiki/Fibonacci_number,
# The method @bench_fib@ aims at benchmarking and asserting the performance of @fib@ implementation (up to a 0.99 precision). It relies on @assert_performance_linear@ (provided by minitest/benchmark) which will invoke the block with increasing values for @n@ (1, 10, 100, 1000, and 10000).

I ask you: what's wrong in this example???

#<{complexity/fib1.rb}

Before answering the question, let me add that @assert_performance_linear@ itself is not wrong or buggy: it calls the block as expected, measure the execution time as expected and simply fails unless the execution time of the block is a linear function of @n@. As you can see below, the test passes which is certainly good as well! 

#<{complexity/fib1_result.sh}

So, what's wrong???


h3. Trivially true or just plain wrong?

Here is what is wrong: the assertion is trivially true (but see later). Therefore the test is perfectly and completely useless (but see later):

#<{complexity/fib2.rb}

Here is why:

# The call to @fib(1000)@ itself has a *constant* execution time _T_ (but see later)
# One call @fib(1000)@ takes _T_ ms., ten calls take _10*T_ ms., ..., ten thousands calls take _10000*T_ ms.
# The linear progression is implied by @assert_performance_linear@ only, and absolutely independent of @fib@

In fact, the code given in the example is somewhat equivalent to (and the test passes, of course):

#<{complexity/fib3.rb}

To summarize: the assertion is trivially true and the test is wrong. No, because the test passes. Then, the assertion is trivially true as well as the test, and the benchmarking approach itself is wrong. Plain wrong?? Or something else?? Aaarrrrgghhh!!!


h3. What do you benchmark??

Unfortunately, things are a bit more complicated than expected. The explanation and truth of all of this story relies and what you want to benchmark precisely. The example is amazing because:

it asserts a lot of things, but certainly not that @fib@ has a linear execution time (aka _O(n)_).

